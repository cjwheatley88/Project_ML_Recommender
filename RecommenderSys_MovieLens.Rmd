---
title: "Movie Recommender System"
author: "_Christopher Wheatley_"
date: "17 September 2022"
geometry: "left=1cm,right=1cm,top=2cm,bottom=2cm"
output:
  pdf_document:
    df_print: kable
    number_sections: yes
    toc: yes
    fig_caption: yes
    includes:
      in_header: preamble.tex
  html_document: default
include-before: '`\newpage{}`{=latex}'
fontsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.align = "center", 
                      out.width = "75%")
```

\newpage
# **Executive Summary**  

This report satisfies the first capstone project of two; within the Data Science program provided by edX and HarvardX. The primary objective being to generate a movie rating recommendation system through data; analysis, visualization, hypothesis generation, optimization and testing. As such; we have proven that given a large enough data set of movie reviews with variables for movie rating, movie genre, user identification, movie identification. One is able to build the following machine learning algorithm which is able to predict a rating for movie i and user u; to a Root Mean Squared Error (RMSE) / accuracy of 0.8704178.
  
  $$y_{hat} = \mu_{ratings} + ({\frac{bias_{genres}}{(n() + \lambda)}}) + ({\frac{bias_{userId}}{(n() + \lambda)}) + (\frac{bias_{movieId}}{(n() + \lambda)})}$$


# **Initial Setup | Verification**

*Setup:*  

Fortunately, the Harvard X Data Science capstone course has provided the necessary code to initialize both the training and validation data sets.

```{r}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")

if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)

library(caret)

library(data.table)

dl <- tempfile()

download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

removed <- anti_join(temp, validation)

edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```
  
*Verification:*  

Let's verify the initialization was a success; analyzing the following data frames:

- 'edx'; which will be our *training* data set.

- 'validation'; which will be our *test* data set.\

The dimensions of the 'edx'/training set; details a data.table of over 9 million observations [m]; with 6 variables [n] associated with each observation. Let's assess the completeness of this data, looking for missing values in each column/variable. 

```{r}

tibble(rows = nrow(edx), variables = ncol(edx))

data.frame(variable = names(edx),
           class = sapply(edx, typeof),
           first_values = sapply(edx, function(x) paste0(head(x, n = 3),  collapse = ", ")),
           row.names = NULL) %>% 
  tibble()

```

\newpage

*Checking for missing Data:*

*Training data*

I like to utilize a for loop across each variable looking for missing data. See code below.

```{r - na check, echo = TRUE}

#example method to assess NAs with for loop.

for (i in 1:ncol(edx)) {
  na <- sum(is.na(edx[,..i]))
  print(na)
}
```

All 6 variables have zero missing values in the training set, lets evaluate our 'validation' (test) set the same way.

*Test / Validation Data*

The dimensions of the 'validation' / test set; details a data.table of just under 1 million observations [m]; with 6 variables [n] associated with each observation. The data.table has identified Nil missing values. 
  
```{r}

tibble(rows = nrow(validation), variables = ncol(validation))

data.frame(variable = names(validation),
           class = sapply(validation, typeof),
           first_values = sapply(validation, function(x) paste0(head(x, n = 3),  collapse = ", ")),
           row.names = NULL) %>% 
  tibble()

for(i in 1:ncol(validation)) {
  na <- sum(is.na(validation[,..i]))
  print(na)
}
```

---


\newpage
# **Objective | Strategy**

*Objective:* 

Develop and test a movie recommendation model able to predict ratings of movie [i] for user[u]. With accuracy measured through a RMSE < 0.86490. 

*Strategy:*  

First examine the relationship between the following variables: 'ratings' [dependent variable] and; 'userId', 'movieId' and 'genres' [independent variables]. The reasoning for choosing only these variables; is for brevity and based off an intuition that in a significant volume of reviews. Each user movie and genre should detail a generalized bias relative to the mean rating. Thus, given new observations with the same independent variables a model can be formed to compute [add the bias terms to a mean] and return a probabilistic estimate of a rating [y_hat].  
  
# **Variable Analysis | Visualization**  

*Variable: rating*

Frequency distribution of ratings.

```{r - visualization, fig.cap="Histogram of Ratings"}

hist(edx$rating, xlab = "Rating")

```
\newpage
*Ratings histogram tabulated:*

```{r}
edx %>% group_by(rating) %>% 
  summarize(count = n()) %>% 
  arrange(., desc(count)) %>% 
  mutate(proportion = round(count/sum(count),2))

```
  
```{r - assign mode variable}

modeTrain <- 4

```
  
To summarize what we have found with the 'rating' variable. The mode is 4 and mean is `r mean(edx$rating)`. Ratings between whole numbers are less frequent then their whole number equivalent. This variable can be utilized in a supervised learning model to provide real outcomes for training a hypothesis. As such, it may be necessary to convert this variable into a factor or category for optimization purposes. 

---

\newpage
*Variable: userId*

Magnitude of unique users.  

```{r}

N_UserTrain <- length(unique(edx$userId))

N_UserTest <- length(unique(validation$userId))

tibble(N_UserTrain = N_UserTrain, 
                     N_UserTest = N_UserTest, 
                     Delta = N_UserTrain - N_UserTest)

```
  
The table above depicts the number of unique users in both the training data set [69978] and the test data set [68534]. Also highlighting the difference [1344] between each.  

*Note:*  
This apparent difference, may cause a problem later in system design/utilization, if we constrain the algorithm to only accept user data evident within the training set. If this is to be the case, our model will be limited to known parameters and could produce NAs. As such and if required, we will replace missing values with a mean bias value for each parameter. I.e.

\
*f(x) = ifelse(user bias missing, then replace with mean(all$userBias), else leave value)*  

Figure 2 - depicts a histogram of reviews from users.  

```{r - freq dist of user reviews, fig.cap="Histogram UserId"}

edx %>% 
  count(userId) %>% 
  ggplot(aes(n)) +
  geom_histogram(bins = 50, color = "black") +
    scale_x_log10()

```
  
The above right skewed plot highlights 'outlier' users, at the higher and lower end of the number of reviews. Regularization may be useful to penalize predictions from users with the largest variance from the mean.  

---

\newpage    
*Variable: movieId*  

Magnitude of unique movies.

```{r}

N_MoviesTrain <- length(unique(edx$movieId))

N_MoviesTest <- length(unique(validation$movieId))

tibble(N_MoviesTrain = N_MoviesTrain, 
                     N_MoviesTest = N_MoviesTest, 
                     Delta = N_MoviesTrain - N_MoviesTest)

```
  
The table above depicts the number of unique movies in both the training data set [10677] and the test data set [9809]. Also highlighting the difference [868] between each. 

Figure 3 - depicts a histogram of the movieId variable:

```{r - movie freq dist, fig.cap="Histogram movieId"}

edx %>% 
  count(movieId) %>% 
  ggplot(aes(n)) +
  geom_histogram(bins = 50, color = "black") +
  scale_x_log10()

```
  
Similar to userId, movieId contains outliers which may also need to be regularized.

---

\newpage
*Variable: genres*

Magnitude of unique genres.

```{r - genres}

edx %>% 
  summarize(genres = n_distinct(genres))

```

For curiosities sake, let's look at the genres with the most reviews.

```{r}

edx %>% 
  group_by(genres) %>% 
  summarize(count = n()) %>% 
  arrange(., desc(count)) %>% 
  top_n(., n = 10)

```
  
Analyzing the 'genres' variable depicts 797 unique categories within the 'edx' data-set. People love Drama... 

\newpage
# **Hypothesis | Method**

A Naive Bayes method will be utilized to form the model hypothesis. This approach starts with the mean rating for all training reviews and adds bias terms in an iterative fashion, assessing with each addition the accuracy of the model. As stated in the objective, accuracy will be measured through RMSE calculations between a training set and one cross validation set. Regularization will be applied where necessary.

\
$$y_{hat} = \mu_{ratings} + bias_{term1} + bias_{term2} + .. n$$

### Stratify Data

As the validation data set is restricted to the final model accuracy assessment. I will split the training data into a training data-set and a cross-validation (CV) data-set. [.9 | .1 respectively]

```{r - stratify, echo = TRUE}

set.seed(1, sample.kind = "Rounding")

index <- createDataPartition(y = edx$rating, times = 1, p = .1, list = FALSE)

train <- edx[-index,]

temp <- edx[index,]

#To avoid our model producing "NA"s; 
#we must ensure the same categorical data is both in the training and cross-validation data-sets.

cv <- temp %>% 
  semi_join(train, by = "movieId") %>% 
  semi_join(train, by = "userId") %>% 
  semi_join(train, by = "genres")

removed <- anti_join(temp, cv)

train <- rbind(train, removed)

rm(removed, temp, index)

```

\newpage
### Iteration 1 - Mode and Mean Prediction

```{r - iteration 1, echo=TRUE}

#Initial model utilizing mode only.

modeOnly <- RMSE(cv$rating, modeTrain)

#Initial model utilizing mean only.

meanOnly <- RMSE(cv$rating, mean(train$rating))

results <- tibble(Model = c("Mode","Mean"),
       RMSE = c(modeOnly, meanOnly))

results

#Mean looks to be a more accurate constant for prediction.

```

\newpage
### Iteration 2 - Adding Genre.

Let's add a bias term for Genre as depicted below.    

$$y_{hat} = \mu_{ratings} + bias_{genres}$$

```{r - iteration 2, echo=TRUE}

#Mean + Genre

muTrain <- mean(train$rating)

genre_effect <- train %>% 
  group_by(genres) %>% 
  summarize(b_g = mean(rating - muTrain))

pred_1 <- cv %>% 
  left_join(genre_effect, by = "genres") %>%
  mutate(y_hat = muTrain + b_g)

muPlusGenre <- RMSE(pred_1$y_hat,cv$rating)

results <- results %>% add_row(Model = "MeanPlusGenre", 
                               RMSE = muPlusGenre)

results

```

With the second iteration - our model has increased accuracy. 

\newpage
### Iteration 3 - Adding userId.

Now let's add another bias term - userId. 

$$y_{hat} = \mu_{ratings} + bias_{genres} + {bias_{userId}}$$
```{r - iteration 3, echo=TRUE}

#Mean + Genre + userId

user_effect <- train %>% 
  left_join(genre_effect, by = "genres") %>% 
  group_by(userId) %>% 
  summarize(b_u = mean(rating - muTrain - b_g))

pred_2 <- cv %>% 
  left_join(genre_effect, by = "genres") %>% 
  left_join(user_effect, by = "userId") %>% 
  mutate(y_hat = muTrain + b_g + b_u)

results <- results %>% add_row(Model = "MeanPlusGenre_PlusUser", 
                               RMSE = RMSE(pred_2$y_hat, cv$rating))

results

```

\newpage
### Iteration 4 - Adding movieId.

Now let's add another bias term - movieId

$$y_{hat} = \mu_{ratings} + bias_{genres} + bias_{userId} + bias_{movieId}$$

```{r - iteration 4, echo=TRUE}

#Mean + Genre + userId + movieId

movie_effect <- train %>% 
  left_join(genre_effect, by = "genres") %>% 
  left_join(user_effect, by = "userId") %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - muTrain - b_g - b_u))

pred_3 <- cv %>% 
  left_join(genre_effect, by = "genres") %>% 
  left_join(user_effect, by = "userId") %>% 
  left_join(movie_effect, by = "movieId") %>% 
  mutate(y_hat = muTrain + b_g + b_u + b_i)

results <- results %>% add_row(Model = "MeanPlusGenre_PlusUser_PlusMovie", 
                               RMSE = RMSE(pred_3$y_hat, cv$rating))

results

```

The model is becoming more accurate, however we are still a ways off from the performance objective. Let's add regularization into the mix.

\newpage
### Iteration 5 - Adding regularization

As highlighted through the analysis and visualization phase. Certain parameter values have significantly less frequent observations, and since we are trying to generalize an average prediction for each dimension. These outlier values can add unwanted variability into our predictions. As such we will add the penalty term 'Lambda' ($\lambda$) to our hypothesis, reducing variability for each bias term.

$$y_{hat} = \mu_{ratings} + ({\frac{bias_{genres}}{(n + \lambda)}}) + ({\frac{bias_{userId}}{(n + \lambda)}) + (\frac{bias_{movieId}}{(n + \lambda)})}$$
  
To optimize our hypothesis and select the most accurate value for $\lambda$, we will iterate values from 4 to 10; in increments of .25. Plotting our results and selecting the $\lambda$ value which returns the minimum RMSE.  
  
```{r - iteration 5, echo=TRUE}

lambdaList <- seq(4, 10, .25)

rmseFinal <- sapply(lambdaList, function(l){
  
  mu <- mean(train) 
  
  genre_effect <- train %>% 
  group_by(genres) %>% 
  summarize(b_g = sum(rating - muTrain)/(n() + l))
  
  user_effect <- train %>% 
  left_join(genre_effect, by = "genres") %>% 
  group_by(userId) %>% 
  summarize(b_u = sum(rating - muTrain - b_g)/(n() + l))
  
  movie_effect <- train %>% 
  left_join(genre_effect, by = "genres") %>% 
  left_join(user_effect, by = "userId") %>% 
  group_by(movieId) %>% 
  summarize(b_i = sum(rating - muTrain - b_g - b_u)/(n() + l))
  
  predFinal <- cv %>% 
  left_join(genre_effect, by = "genres") %>% 
  left_join(user_effect, by = "userId") %>% 
  left_join(movie_effect, by = "movieId") %>% 
  mutate(y_hat = muTrain + b_g + b_u + b_i) %>% 
  pull(y_hat)
  
  return(RMSE(predFinal, cv$rating))
  
})

```

\newpage
*Optimization Plot*

Looking at the below plot, Lambda set to `r lambdaList[which.min(rmseFinal)]`, produces the highest performing model. With a RMSE score of `r min(rmseFinal)`. 

```{r - optimization plot, fig.cap="Optimization Plot - Lambda Values"}

plot_tibble <- tibble(rmse = rmseFinal, lambda = lambdaList)

plot_tibble %>% ggplot() +
  geom_point(aes(x = lambda, y = rmse))

results <- results %>% add_row(Model = "MeanPlusGenre_PlusUser_PlusMovie_Regularized", 
                               RMSE = min(rmseFinal))

results

```

We haven't quite met the accuracy objective of < 0.86490. Delta = `r (min(rmseFinal)) - .86490`.  

\newpage
# **Hypothesis Testing** 

As the submission deadline is getting closer, I will implement our last iteration hypothesis on the validation data set and submit the project. Given more time; I would continue to add and evaluate new bias terms.  
  
```{r - hypothesis testing}

lambda <- lambdaList[which.min(rmseFinal)]

rmseTest <- function(l){

muFinal <- mean(edx$rating)

b_g <- edx %>% 
  group_by(genres) %>% 
  summarize(b_g = sum(rating - muFinal)/(n() + l))

b_u <- edx %>% 
  left_join(b_g, by = "genres") %>% 
  group_by(userId) %>% 
  summarize(b_u = sum(rating - muFinal - b_g)/(n()+ l))

b_i <- edx %>% 
  left_join(b_g, by = "genres") %>% 
  left_join(b_u, by = "userId") %>% 
  group_by(movieId) %>% 
  summarize(b_i = sum(rating - muFinal - b_g - b_u)/(n() + l))

pred <- validation %>% 
  left_join(b_g, by = "genres") %>% 
  left_join(b_u, by = "userId") %>% 
  left_join(b_i, by = "movieId") %>% 
  mutate(pred = muFinal + b_g + b_u + b_i) %>% 
  pull(pred)

return(RMSE(pred, validation$rating))

}

rmse <- rmseTest(lambda)

results <- results %>% add_row(Model = "Validation", RMSE = rmse)

results

```
  
# **Result**

Our hypothesis has achieved a 'test' RMSE accuracy of: RMSE = `r rmse`.

# **Conclusion**

It's nice to think back and appreciate how far I have come, this has been a very challenging yet rewarding program and project. Given more time, I would have liked to add additional bias terms or dimensions to the hypothesis. And given more computational power, a matrix factorization approach would be an interesting and rewarding alternative approach. Thankyou Edx and HarvardX.