---
title: "Movie Recommender System"
author: "_Christopher Wheatley_"
date: "17 September 2022"
geometry: margin = 1cm
output:
  pdf_document:
    df_print: kable
    number_sections: yes
    toc: yes
    fig_caption: yes
    includes:
      in_header: preamble.tex
  html_document: default
include-before: '`\newpage{}`{=latex}'
fontsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.align = "center", 
                      out.width = "75%")
```

\newpage
# **Executive Summary**  

This report satisfies the first capstone project of two; within the Data Science program provided by edX and HarvardX. The primary objective being to generate a movie rating recommendation system through data; analysis, visualization, hypothesis generation, optimization and testing. As such; we have proven that given a large enough data set of movie reviews with variables for movie rating, movie genre, user identification, movie identification. One is able to build the following generalized machine learning algorithm which is able to predict a rating for movie i and user u; to a Root Mean Squared Error (RMSE) / accuracy of 0.8704178.
  
  $$y_{hat} = \mu_{ratings} + ({\frac{bias_{genres}}{(n() + \lambda)}}) + ({\frac{bias_{userId}}{(n() + \lambda)}) + (\frac{bias_{movieId}}{(n() + \lambda)})}$$


# **Initial Setup | Verification**

*Setup:*  

Fortunately, the Harvard X Data Science capstone course has provided the necessary code to initialize both the training and validation data sets.

```{r}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")

if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)

library(caret)

library(data.table)

dl <- tempfile()

download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

removed <- anti_join(temp, validation)

edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```
  
*Verification:*  

Let's verify the initialization was a success; analyzing the following data frames:

- 'edx'; which will be our *training* data set.

- 'validation'; which will be our *test* data set.\

```{r}

tibble(rows = nrow(edx), variables = ncol(edx))

data.frame(variable = names(edx),
           class = sapply(edx, typeof),
           first_values = sapply(edx, function(x) paste0(head(x, n = 3),  collapse = ", ")),
           row.names = NULL) %>% 
  tibble()

```
The dimensions of the 'edx'/training set; details a data.table of over 9 million observations [m]; with 6 variables [n] associated with each observation. Let's assess the completeness of this data, looking for missing values in each column/variable. 
\newpage

*Checking for missing Data:*

I like to utilize a for loop across each variable looking for missing data.

```{r - na check, echo = TRUE}

#example method to assess NAs with for loop.

for (i in 1:ncol(edx)) {
  na <- sum(is.na(edx[,..i]))
  print(na)
}
```

All 6 variables have zero missing values in the training set, lets evaluate our 'validation' (test) set the same way.
  
```{r}

tibble(rows = nrow(validation), variables = ncol(validation))

data.frame(variable = names(validation),
           class = sapply(validation, typeof),
           first_values = sapply(validation, function(x) paste0(head(x, n = 3),  collapse = ", ")),
           row.names = NULL) %>% 
  tibble()

for(i in 1:ncol(validation)) {
  na <- sum(is.na(validation[,..i]))
  print(na)
}
```
The dimensions of the 'validation' / test set; details a data.table of just under 1 million observations [m]; with 6 variables [n] associated with each observation. The data.table has identified Nil missing values.  

---

# **Objective | Strategy**

*Objective:* 

Develop and test a movie recommendation model able to predict ratings of movie [i] for user[u]. With accuracy measured through a RMSE < 0.86490. 

*Strategy:*  

First examine the relationship between the following variables: 'ratings' [dependent variable] and; 'userId', 'movieId' and 'genres' [independent variables]. The reasoning for choosing only these variables; is for brevity and based off an intuition that in a significant volume of reviews. Each user movie and genre should detail a generalized bias relative to the mean rating. Thus, given new observations with the same independent variables a model can be formed to compute [add the bias terms to a mean] and return a probabilistic estimate of a rating [y_hat].  
  
# **Variable Analysis | Visualization**  

*Variable: rating*

Frequency distribution of ratings.\  

```{r - visualization, fig.cap="Histogram of Ratings"}
hist(edx$rating, 
     main = "Histogram of Ratings",
     xlab = "Rating")
```
\newpage  
Numerical representation of the above histogram is as follows:

```{r - tabulated histogram with proportion, fig.cap="Ratings Table"}
edx %>% group_by(rating) %>% 
  summarize(count = n()) %>% 
  arrange(., desc(count)) %>% 
  mutate(proportion = round(count/sum(count),2))

```
  
```{r - assign mode variable}

modeTrain <- 4

```
  
To summarize what we have found with the 'rating' variable. The mode is 4 and mean is `r mean(edx$rating)`. Ratings between whole numbers are less frequent then their whole number equivalent. This variable can be utilized in a supervised learning model to provide real outcomes for training a hypothesis. As such, it may be necessary to convert this variable into a factor or category for optimization purposes.  
  
*Variable: userId*

Magnitude of unique users.  

```{r}

N_UserTrain <- length(unique(edx$userId))

N_UserTest <- length(unique(validation$userId))

tibble(N_UserTrain = N_UserTrain, 
                     N_UserTest = N_UserTest, 
                     Delta = N_UserTrain - N_UserTest)

```
  
The table above depicts the number of unique users in both the training data set [69978] and the test data set [68534]. Also highlighting the difference [1344] between each. This apparent difference, may cause a problem later in the system design/utilization, if we constrain the algorithm to only users seen within the training set; and our model is required to take on new user data. If this is to be the case; we will replace missing values with the mean bias value for each parameter. I.e.  

`ifelse(user bias missing, then replace with mean(all$userBias), else leave value)`  
  
Now let's plot the frequency distribution of reviews from users.  

```{r - freq dist of user reviews, fig.cap="Histogram UserId"}

edx %>% 
  count(userId) %>% 
  ggplot(aes(n)) +
  geom_histogram(bins = 50, color = "black") +
  ggtitle("Histogram UserId") +
  scale_x_log10()

```
  
The above right skewed plot highlights 'outlier' users, at the higher and lower end of the number of reviews. Regularization may be useful to penalize predictions from users with the largest variance from the mean.  

---
    
*Variable: movieId*  

Number of unique movies between data sets and delta.

```{r}

N_MoviesTrain <- length(unique(edx$movieId))

N_MoviesTest <- length(unique(validation$movieId))

tibble(N_MoviesTrain = N_MoviesTrain, 
                     N_MoviesTest = N_MoviesTest, 
                     Delta = N_MoviesTrain - N_MoviesTest)

```
  
Frequency of 'movieId' within training data-set:

```{r - movie freq dist, fig.cap="Histogram movieId"}

edx %>% 
  count(movieId) %>% 
  ggplot(aes(n)) +
  geom_histogram(bins = 50, color = "black") +
  ggtitle("Histogram movieId") +
  scale_x_log10()

```
  
Similar to userId, movieId contains outliers; which may need to be penalized also.

---

*Variable: genres*

```{r - genres}

edx %>% 
  summarize(genres = n_distinct(genres))

edx %>% 
  group_by(genres) %>% 
  summarize(count = n()) %>% 
  arrange(., desc(count)) %>% 
  top_n(., n = 10)

```
  
Analyzing the 'genres' variable depicts 797 unique categories within the 'edx' data-set. People love Drama... 

# **Hypothesis | Method**

A Naive Bayes method will be utilized to form the model hypothesis. This approach starts with the mean rating of all reviews and adds bias terms in an iterative fashion, assessing with each addition the accuracy of the model. As stated in the objective, accuracy will be measured through RMSE calculations between a training set and one cross validation set. Regularization will be applied where necessary.  

```{r}

#Stratify data into a training data-set and a cross-validation (CV) data-set. [.9 | .1 respectively]

set.seed(1, sample.kind = "Rounding")

index <- createDataPartition(y = edx$rating, times = 1, p = .1, list = FALSE)

train <- edx[-index,]

temp <- edx[index,]

#To avoid our model producing "NA"s; we must ensure the same categorical data is both in the training and cross-validation data-sets.

cv <- temp %>% 
  semi_join(train, by = "movieId") %>% 
  semi_join(train, by = "userId") %>% 
  semi_join(train, by = "genres")

removed <- anti_join(temp, cv)

train <- rbind(train, removed)

rm(removed, temp, index)

#Initial model utilizing mode only.

modeOnly <- RMSE(cv$rating, modeTrain)

#Initial model utilzing mean only.

meanOnly <- RMSE(cv$rating, mean(train$rating))

results <- tibble(Model = c("Mode","Mean"),
       RMSE = c(modeOnly, meanOnly))

results

#Mean looks to be a more accurate means of prediction.

#As such, our model will start with the mean estimate plus Bias terms for the variables: genres, movieId and userId. Let's begin with generating least square estimates for each genre.

```

$$y_{hat} = \mu_{ratings} + bias_{term1} + bias_{term2} + .. n$$
  
Let's add a bias term for Genre with our first iteration.    

$$y_{hat} = \mu_{ratings} + bias_{genres}$$

```{r}

#Mean + Genre

muTrain <- mean(train$rating)

genre_effect <- train %>% 
  group_by(genres) %>% 
  summarize(b_g = mean(rating - muTrain))

pred_1 <- cv %>% 
  left_join(genre_effect, by = "genres") %>%
  mutate(y_hat = muTrain + b_g)

muPlusGenre <- RMSE(pred_1$y_hat,cv$rating)

results <- results %>% add_row(Model = "MeanPlusGenre", RMSE = muPlusGenre)

results

```

With the first iteration - our model has increased accuracy. Now let's add another bias term - userId. 

$$y_{hat} = \mu_{ratings} + bias_{genres} + {bias_{userId}}$$
```{r}

#Mean + Genre + userId

user_effect <- train %>% 
  left_join(genre_effect, by = "genres") %>% 
  group_by(userId) %>% 
  summarize(b_u = mean(rating - muTrain - b_g))

pred_2 <- cv %>% 
  left_join(genre_effect, by = "genres") %>% 
  left_join(user_effect, by = "userId") %>% 
  mutate(y_hat = muTrain + b_g + b_u)

RMSE(pred_2$y_hat, cv$rating)

results <- results %>% add_row(Model = "MeanPlusGenre_PlusUser", RMSE = RMSE(pred_2$y_hat, cv$rating))

results

```

The next bias term is for movies.

$$y_{hat} = \mu_{ratings} + bias_{genres} + bias_{userId} + bias_{movieId}$$

```{r}

#Mean + Genre + userId + movieId

movie_effect <- train %>% 
  left_join(genre_effect, by = "genres") %>% 
  left_join(user_effect, by = "userId") %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - muTrain - b_g - b_u))

pred_3 <- cv %>% 
  left_join(genre_effect, by = "genres") %>% 
  left_join(user_effect, by = "userId") %>% 
  left_join(movie_effect, by = "movieId") %>% 
  mutate(y_hat = muTrain + b_g + b_u + b_i)

RMSE(pred_3$y_hat, cv$rating)

results <- results %>% add_row(Model = "MeanPlusGenre_PlusUser_PlusMovie", RMSE = RMSE(pred_3$y_hat, cv$rating))

results

```

As highlighted through the analysis and visualization phase. Certain parameter values have significantly less frequent observations, and since we are trying to generalize an average prediction for each dimension. These outlier values could add unwanted variability with our predictions. As such we will add the penalty term 'Lambda' to our hypothesis, reducing the outlier affect from less frequent values skewing our predictions. 

$$y_{hat} = \mu_{ratings} + ({\frac{bias_{genres}}{(n + \lambda)}}) + ({\frac{bias_{userId}}{(n + \lambda)}) + (\frac{bias_{movieId}}{(n + \lambda)})}$$
  
To optimize our hypothesis and select the most accurate value for $\lambda$, we will iterate values from 4 to 10; in increments of .25. Plotting our results and selecting the $\lambda$ value which returns the minimum RMSE.  
  
```{r}

lambdaList <- seq(4, 10, .25)

rmseFinal <- sapply(lambdaList, function(l){
  
  mu <- mean(train) 
  
  genre_effect <- train %>% 
  group_by(genres) %>% 
  summarize(b_g = sum(rating - muTrain)/(n() + l))
  
  user_effect <- train %>% 
  left_join(genre_effect, by = "genres") %>% 
  group_by(userId) %>% 
  summarize(b_u = sum(rating - muTrain - b_g)/(n() + l))
  
  movie_effect <- train %>% 
  left_join(genre_effect, by = "genres") %>% 
  left_join(user_effect, by = "userId") %>% 
  group_by(movieId) %>% 
  summarize(b_i = sum(rating - muTrain - b_g - b_u)/(n() + l))
  
  predFinal <- cv %>% 
  left_join(genre_effect, by = "genres") %>% 
  left_join(user_effect, by = "userId") %>% 
  left_join(movie_effect, by = "movieId") %>% 
  mutate(y_hat = muTrain + b_g + b_u + b_i) %>% 
  pull(y_hat)
  
  return(RMSE(predFinal, cv$rating))
  
})

plot_tibble <- tibble(rmse = rmseFinal, lambda = lambdaList)

plot_tibble %>% ggplot() +
  geom_point(aes(x = lambda, y = rmse))

results <- results %>% add_row(Model = "MeanPlusGenre_PlusUser_PlusMovie_Regularized", RMSE = min(rmseFinal))

results

```
  
Lambda set to `r lambdaList[which.min(rmseFinal)]`, produces the highest performing model. With a RMSE score of `r min(rmseFinal)`.  

We haven't quite met the accuracy objective of < 0.86490. 

Delta = `r (min(rmseFinal)) - .86490`.  

As the submission deadline is getting closer, I will implement this hypothesis on the validation data set and submit the project. Given more time; I would continue to add and evaluate new bias terms.  
  
```{r}

lambda <- lambdaList[which.min(rmseFinal)]

rmseTest <- function(l){

muFinal <- mean(edx$rating)

b_g <- edx %>% 
  group_by(genres) %>% 
  summarize(b_g = sum(rating - muFinal)/(n() + l))

b_u <- edx %>% 
  left_join(b_g, by = "genres") %>% 
  group_by(userId) %>% 
  summarize(b_u = sum(rating - muFinal - b_g)/(n()+ l))

b_i <- edx %>% 
  left_join(b_g, by = "genres") %>% 
  left_join(b_u, by = "userId") %>% 
  group_by(movieId) %>% 
  summarize(b_i = sum(rating - muFinal - b_g - b_u)/(n() + l))

pred <- validation %>% 
  left_join(b_g, by = "genres") %>% 
  left_join(b_u, by = "userId") %>% 
  left_join(b_i, by = "movieId") %>% 
  mutate(pred = muFinal + b_g + b_u + b_i) %>% 
  pull(pred)

return(RMSE(pred, validation$rating))

}

rmse <- rmseTest(lambda)

```
  
# **Result**

Utilizing a Naive Bayes approach - the following model achieved a 'test' RMSE accuracy of: RMSE = `r rmse`.

# **Conclusion**

Given more time, I would have liked to add additional bias terms or dimensions to the hypothesis. Given more computational power, a matrix factorization algorithm would have been interesting to work on. 