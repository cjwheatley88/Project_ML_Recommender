---
title: "Movie Recommender System"
author: "_Christopher Wheatley_"
date: "15 September 2022"
output:
  pdf_document:
    df_print: kable
    number_sections: yes
    toc: yes
    fig_caption: yes
    includes:
      in_header: preamble.tex
  html_document: default
include-before: '`\newpage{}`{=latex}'
fontsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.align = "center", 
                      out.width = "75%")
```

\newpage

# **Executive Summary:**  
Blah blah

---

**Initial Setup:**

Fortunately, the Harvard X Data Science capstone course has provided the necessary code to initialize both the training and validation data sets.

```{r}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")

if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)

library(caret)

library(data.table)

dl <- tempfile()

download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

removed <- anti_join(temp, validation)

edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```

\newpage
  
# **Verification:**
Let's verify the initialization was a success; analyzing the following data frames:

- edx == "training" data set.

- validation == "test" data set.

*Note. Our validation data set is not to be utilized for adjusting our model, it is reserved for generating our final predictions and assessing accuracy, as measured with the 'Root Mean Square Error' (RMSE) method. 

```{r}
dim(edx)

str(edx)
```
The dimensions of the 'edx'/training set; details a data.table of over 9 million observations [m]; with 6 variables associated with each observation. Let's assess the completeness of this data, looking for missing values in each column/variable. 

```{r}
for (i in 1:ncol(edx)) {
  na <- sum(is.na(edx[,..i]))
  print(na)
}
```

Nil missing values in the training set, lets evaluate our 'validation' (test) set the same way.

```{r}

dim(validation)

str(validation)

for(i in 1:ncol(validation)) {
  na <- sum(is.na(validation[,..i]))
  print(na)
}
```
The dimensions of the 'validation'/test set; details a data.table of just under 1 million observations [m]; with 6 variables associated with each observation. The data.table has identified Nil missing values.  

---

# **Strategy | Objective**
Since the objective of this project is to develop a recommendation system able to predict ratings of movie [i] for user[u].
Lets examine the relationship between ratings [dependent variable] and; userId, movieId and genres [independent variables].   My reasoning for choosing only these variables; is for brevity and also based off the intuition that in a significant volume of reviews. Each user, movie and genre should detail a generalized bias relative to our prediction. As such, given new observations with the same independent variables a model can be formed to compute and add these bias terms and return a probabilistic estimate of a rating [y_hat]. 
  
\newpage

# **Variable Analysis and Visualization**  

Variable: rating

Frequency distribution of ratings.\  

```{r}
hist(edx$rating, 
     main = "Histogram of Ratings",
     xlab = "Rating")
```
  
Numerical representation of the above histogram is as follows:

```{r}
edx %>% group_by(rating) %>% 
  summarize(count = n()) %>% 
  arrange(., desc(count)) %>% 
  mutate(proportion = round(count/sum(count),2))

```
The most common value for ratings is 4.

```{r}

modeTrain <- 4  

```

Lets look at the mean rating.

```{r}

muTrain <- mean(edx$rating)

muTrain

```
To summarize, what we have found with the 'rating' variable. The mode is 4, mean is 3.512. Ratings between whole numbers are less frequent then their whole number equivalent. This variable can be utilized in a supervised learning model to provide real outcomes to train on. As such, it may be necessary to convert this variable into a factor or category for optimization purposes.  

---
  
Variable: userId

Magnitude of unique users.  

```{r}

N_UserTrain <- length(unique(edx$userId))

N_UserTest <- length(unique(validation$userId))

tibble(N_UserTrain = N_UserTrain, 
                     N_UserTest = N_UserTest, 
                     Delta = N_UserTrain - N_UserTest)

```

The table above depicts the number of unique users in both the training data set [69978] test data set [68534]. Also highlighting the difference [1344] between each. This apparent difference, may cause a problem later in the system design/utilization, if we constrain the algorithm to only users seen within the training set; and our model is required to take on new user data. If this is to be the case; we will replace missing values with the mean bias value for each parameter.I.e.  

`ifelse(user bias missing / == NA, then mean(all$userBias), else leave value)`  

Let's look at the number of reviews for each user.

```{r}

edx %>% 
  group_by(userId) %>% 
  summarize(count = n()) %>% 
  arrange(., desc(count)) %>% 
  slice_head(n = 10)

edx %>% 
  group_by(userId) %>% 
  summarize(count = n()) %>% 
  arrange(., desc(count)) %>%
  slice_tail(n = 10)

```
Next we will assess the distribution of user reviews.\

```{r}

edx %>% 
  count(userId) %>% 
  ggplot(aes(n)) +
  geom_histogram(bins = 50, color = "black") +
  ggtitle("Histogram UserId") +
  scale_x_log10()

```
  
The above plot highlights certain 'outlier' users, at the higher end of total reviews. Regularization may be useful to penalize predictions with users with the largest variance.  

---
    
Variable: movieId  

Magnitude of unique movies.

```{r}

N_MoviesTrain <- length(unique(edx$movieId))

N_MoviesTest <- length(unique(validation$movieId))

tibble(N_MoviesTrain = N_MoviesTrain, 
                     N_MoviesTest = N_MoviesTest, 
                     Delta = N_MoviesTrain - N_MoviesTest)

```

Frequency of 'movieId' within training data-set:

```{R}

edx %>% 
  count(movieId) %>% 
  ggplot(aes(n)) +
  geom_histogram(bins = 50, color = "black") +
  ggtitle("Histogram movieId") +
  scale_x_log10()

```

Frequency range of movie reviews:

```{r}

edx %>% 
  group_by(title) %>% 
  summarize(count = n()) %>% 
  arrange(., desc(count)) %>% 
  slice_head(n = 10)

edx %>% 
  group_by(title) %>% 
  summarize(count = n()) %>% 
  arrange(., desc(count)) %>% 
  slice_tail(n = 10)
```

Similar to userId, movieId may need to be penalized based off the number of reviews observed within the training data set. 

---

Variable: genres

```{r}

edx %>% 
  summarize(genres = n_distinct(genres))

edx %>% 
  group_by(genres) %>% 
  summarize(count = n()) %>% 
  arrange(., desc(count)) %>% 
  top_n(., n = 10)

```
  
Analyzing the 'genres' variable depicts 797 unique categories within the 'edx' data-set. People love Drama... 

# **Hypothesis and Method**

For simplicity sake and for brevity. I will choose a Naive Bayes approach to generating the recommender system model. This approach starts with the mean rating of all reviews and adds bias terms in an iterative fashion, assessing with each addition the accuracy of the model. Accuracy will be measured through RMSE calculations between a training set and one cross validation set. Regularization will be applied where necessary.  

```{r}

#Stratify data into a training data-set and a cross-validation (CV) data-set. [.9 | .1 respectively]

set.seed(1, sample.kind = "Rounding")

index <- createDataPartition(y = edx$rating, times = 1, p = .1, list = FALSE)

train <- edx[-index,]

temp <- edx[index,]

#To avoid our model producing "NA"s; we must ensure parameter data is both in the training and cross-validation data-sets.

cv <- temp %>% 
  semi_join(train, by = "movieId") %>% 
  semi_join(train, by = "userId") %>% 
  semi_join(train, by = "genres")

removed <- anti_join(temp, cv)

train <- rbind(train, removed)

rm(removed, temp, index)

#Initial model utilizing mode only.

modeOnly <- RMSE(cv$rating, modeTrain)

#Initial model utilzing mean only.

meanOnly <- RMSE(cv$rating, muTrain)

results <- tibble(Model = c("Mode","Mean"),
       RMSE = c(modeOnly, meanOnly))

results

#Mean looks to be a more accurate means of prediction.

#As such, our model will start with the mean estimate plus Bias terms for the variables: genres, movieId and userId. Let's begin with generating least square estimates for each genre.

```

$$y_{hat} = \mu_{ratings} + bias_{term1} + bias_{term2} + .. n$$
  
Let's add a bias term for Genre with our first iteration.    

$$y_{hat} = \mu_{ratings} + bias_{genres}$$

```{r}

#Mean + Genre

genre_effect <- train %>% 
  group_by(genres) %>% 
  summarize(b_g = mean(rating - muTrain))

head(genre_effect)

y_hat1 <- cv %>% 
  left_join(genre_effect, by = "genres") %>% 
  mutate(mu = muTrain) %>% 
  summarize(y_hat = mu + b_g)

muPlusGenre <- RMSE(y_hat1$y_hat,cv$rating)

results <- results %>% add_row(Model = "MeanPlusGenre", RMSE = muPlusGenre)

results

```

We seem to be tracking in a positive direction, let's add another bias term - userId. As observed through our exploratory data analysis, certain users are seen to be outliers with reference to the majority; specifically in terms of their frequency of reviews. As such we will apply regularization to this term to reduce the overall variability of our bias term and hopefully increase the accuracy of predictions.  

$$y_{hat} = \mu_{ratings} + bias_{genres} + \frac{bias_{userId}}{(n + \lambda)}$$
```{r}

#Mean + Genre + Regularized userId

user_effect <- train %>% 
  group_by(userId) %>% 
  summarize(b_u = mean(rating - muTrain))

head(user_effect)

#initialize lambda == 2

lambda <- 2

y_hat2 <- cv %>% 
  left_join(genre_effect, by = "genres") %>% 
  left_join(user_effect, by = "userId") %>% 
  mutate(mu = muTrain) %>% 
  group_by(userId) %>% 
  summarize(y_hat = mu + b_g + (b_u/(n() + lambda)))

RMSE(y_hat2$y_hat, cv$rating)

#Let's optimize the penalty term 'lambda'

lambdaList <- seq(0, 20, .25)

RMSEs <- sapply(lambdaList, function(l) {
  
  user_effect_reg <- train %>% 
    group_by(userId) %>% 
    summarize(b_u_reg = sum(rating - muTrain)/(n() + l))
  
  y_hat <- cv %>% 
    left_join(genre_effect, by = "genres") %>% 
    left_join(user_effect_reg, by = "userId") %>% 
    mutate(mu = muTrain) %>%
    summarize(y_hat = mu + b_g + b_u_reg) %>% 
    pull()
  
  return(RMSE(y_hat, cv$rating))
})

userTibble <- tibble(RMSE = RMSEs, Lambda = lambdaList)

plot(userTibble$RMSE, xlab = "Lambda")

which.min(userTibble$RMSE)

min(RMSEs)

```
Lambda set to `r which.min(userTibble$RMSE)`, produces the highest performing model. With a RMSE score of `r min(RMSEs)`


```{r}

results <- results %>% add_row(Model = "MeanPlusGenre_PlusUserRegularized", RMSE = min(RMSEs))

results

```

In finale, let's add a regularized bias term for each movie to our model.  

$$y_{hat} = \mu_{ratings} + bias_{genres} + {\frac{bias_{userId}}{(n + \lambda)} + \frac{bias_{movieId}}{(n + \lambda)}}$$

```{r}

#Mean + Genre + Regularized userId + Regularized movieId

lambdaList <- seq(0, 20, .25)

RMSEs <- sapply(lambdaList, function(l) {
  
  user_effect_reg <- train %>% 
    group_by(userId) %>% 
    summarize(b_u_reg = sum(rating - muTrain)/(n() + 38))
  
  movie_effect_reg <- train %>% 
    group_by(movieId) %>% 
    summarize(b_m_reg = sum(rating - muTrain)/(n() + l))
  
  y_hat <- cv %>% 
    left_join(genre_effect, by = "genres") %>% 
    left_join(user_effect_reg, by = "userId") %>%
    left_join(movie_effect_reg, by = "movieId") %>% 
    mutate(mu = muTrain) %>%
    summarize(y_hat = mu + b_g + b_u_reg + b_m_reg) %>% 
    pull()
  
  return(RMSE(y_hat, cv$rating))
})

userTibble <- tibble(RMSE = RMSEs, Lambda = lambdaList)

plot(userTibble$RMSE, xlab = "Lambda")

which.min(userTibble$RMSE)

min(RMSEs)

```

# **Result**

Utilizing a Naive Bayes approach and with the following variables available we have achieved a RMSE accuracy score of XXXXX

```{r}

#Plot results

```

# **Conclusion**

Limitations / Future work.. given enough computing power, i am eager to utilize a matri